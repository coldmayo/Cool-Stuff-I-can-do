{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me trying to learn deep learning with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "!pip install -q tensorflow-datasets\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset to be used\n",
    "#The dataset is basically a bunch of handwritten numbers\n",
    "mnist = tf.keras.datasets.mnist\n",
    "#x-train and x-test are appart of the training set and y-test and y-train are appart of the test set\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - ETA: 13:53 - loss: 2.3682 - accuracy: 0.093 - ETA: 54s - loss: 1.9108 - accuracy: 0.3713  - ETA: 28s - loss: 1.5449 - accuracy: 0.545 - ETA: 20s - loss: 1.3048 - accuracy: 0.614 - ETA: 16s - loss: 1.1411 - accuracy: 0.670 - ETA: 13s - loss: 1.0220 - accuracy: 0.707 - ETA: 12s - loss: 0.9508 - accuracy: 0.725 - ETA: 11s - loss: 0.8850 - accuracy: 0.746 - ETA: 10s - loss: 0.8393 - accuracy: 0.758 - ETA: 9s - loss: 0.7975 - accuracy: 0.768 - ETA: 8s - loss: 0.7592 - accuracy: 0.78 - ETA: 8s - loss: 0.7267 - accuracy: 0.79 - ETA: 8s - loss: 0.7012 - accuracy: 0.79 - ETA: 7s - loss: 0.6789 - accuracy: 0.80 - ETA: 7s - loss: 0.6531 - accuracy: 0.80 - ETA: 7s - loss: 0.6345 - accuracy: 0.81 - ETA: 6s - loss: 0.6200 - accuracy: 0.82 - ETA: 6s - loss: 0.6052 - accuracy: 0.82 - ETA: 6s - loss: 0.5909 - accuracy: 0.82 - ETA: 6s - loss: 0.5782 - accuracy: 0.83 - ETA: 6s - loss: 0.5670 - accuracy: 0.83 - ETA: 6s - loss: 0.5570 - accuracy: 0.83 - ETA: 5s - loss: 0.5483 - accuracy: 0.84 - ETA: 5s - loss: 0.5409 - accuracy: 0.84 - ETA: 5s - loss: 0.5354 - accuracy: 0.84 - ETA: 5s - loss: 0.5275 - accuracy: 0.84 - ETA: 5s - loss: 0.5191 - accuracy: 0.84 - ETA: 5s - loss: 0.5117 - accuracy: 0.85 - ETA: 5s - loss: 0.5048 - accuracy: 0.85 - ETA: 5s - loss: 0.4978 - accuracy: 0.85 - ETA: 5s - loss: 0.4915 - accuracy: 0.85 - ETA: 5s - loss: 0.4840 - accuracy: 0.85 - ETA: 4s - loss: 0.4766 - accuracy: 0.86 - ETA: 4s - loss: 0.4696 - accuracy: 0.86 - ETA: 4s - loss: 0.4639 - accuracy: 0.86 - ETA: 4s - loss: 0.4583 - accuracy: 0.86 - ETA: 4s - loss: 0.4517 - accuracy: 0.86 - ETA: 4s - loss: 0.4464 - accuracy: 0.86 - ETA: 4s - loss: 0.4424 - accuracy: 0.87 - ETA: 4s - loss: 0.4373 - accuracy: 0.87 - ETA: 4s - loss: 0.4336 - accuracy: 0.87 - ETA: 4s - loss: 0.4295 - accuracy: 0.87 - ETA: 4s - loss: 0.4266 - accuracy: 0.87 - ETA: 4s - loss: 0.4227 - accuracy: 0.87 - ETA: 4s - loss: 0.4188 - accuracy: 0.87 - ETA: 3s - loss: 0.4154 - accuracy: 0.87 - ETA: 3s - loss: 0.4119 - accuracy: 0.87 - ETA: 3s - loss: 0.4081 - accuracy: 0.88 - ETA: 3s - loss: 0.4040 - accuracy: 0.88 - ETA: 3s - loss: 0.3999 - accuracy: 0.88 - ETA: 3s - loss: 0.3959 - accuracy: 0.88 - ETA: 3s - loss: 0.3914 - accuracy: 0.88 - ETA: 3s - loss: 0.3882 - accuracy: 0.88 - ETA: 3s - loss: 0.3865 - accuracy: 0.88 - ETA: 3s - loss: 0.3848 - accuracy: 0.88 - ETA: 3s - loss: 0.3818 - accuracy: 0.88 - ETA: 3s - loss: 0.3787 - accuracy: 0.88 - ETA: 3s - loss: 0.3765 - accuracy: 0.88 - ETA: 3s - loss: 0.3737 - accuracy: 0.89 - ETA: 2s - loss: 0.3709 - accuracy: 0.89 - ETA: 2s - loss: 0.3697 - accuracy: 0.89 - ETA: 2s - loss: 0.3670 - accuracy: 0.89 - ETA: 2s - loss: 0.3639 - accuracy: 0.89 - ETA: 2s - loss: 0.3615 - accuracy: 0.89 - ETA: 2s - loss: 0.3589 - accuracy: 0.89 - ETA: 2s - loss: 0.3574 - accuracy: 0.89 - ETA: 2s - loss: 0.3540 - accuracy: 0.89 - ETA: 2s - loss: 0.3518 - accuracy: 0.89 - ETA: 2s - loss: 0.3499 - accuracy: 0.89 - ETA: 2s - loss: 0.3476 - accuracy: 0.89 - ETA: 2s - loss: 0.3460 - accuracy: 0.89 - ETA: 2s - loss: 0.3439 - accuracy: 0.89 - ETA: 2s - loss: 0.3424 - accuracy: 0.89 - ETA: 1s - loss: 0.3404 - accuracy: 0.90 - ETA: 1s - loss: 0.3381 - accuracy: 0.90 - ETA: 1s - loss: 0.3364 - accuracy: 0.90 - ETA: 1s - loss: 0.3344 - accuracy: 0.90 - ETA: 1s - loss: 0.3327 - accuracy: 0.90 - ETA: 1s - loss: 0.3318 - accuracy: 0.90 - ETA: 1s - loss: 0.3298 - accuracy: 0.90 - ETA: 1s - loss: 0.3281 - accuracy: 0.90 - ETA: 1s - loss: 0.3264 - accuracy: 0.90 - ETA: 1s - loss: 0.3246 - accuracy: 0.90 - ETA: 1s - loss: 0.3231 - accuracy: 0.90 - ETA: 1s - loss: 0.3220 - accuracy: 0.90 - ETA: 1s - loss: 0.3202 - accuracy: 0.90 - ETA: 1s - loss: 0.3191 - accuracy: 0.90 - ETA: 1s - loss: 0.3173 - accuracy: 0.90 - ETA: 1s - loss: 0.3165 - accuracy: 0.90 - ETA: 0s - loss: 0.3153 - accuracy: 0.90 - ETA: 0s - loss: 0.3142 - accuracy: 0.90 - ETA: 0s - loss: 0.3127 - accuracy: 0.90 - ETA: 0s - loss: 0.3111 - accuracy: 0.90 - ETA: 0s - loss: 0.3096 - accuracy: 0.90 - ETA: 0s - loss: 0.3080 - accuracy: 0.90 - ETA: 0s - loss: 0.3065 - accuracy: 0.91 - ETA: 0s - loss: 0.3053 - accuracy: 0.91 - ETA: 0s - loss: 0.3037 - accuracy: 0.91 - ETA: 0s - loss: 0.3027 - accuracy: 0.91 - ETA: 0s - loss: 0.3016 - accuracy: 0.91 - ETA: 0s - loss: 0.3006 - accuracy: 0.91 - ETA: 0s - loss: 0.2997 - accuracy: 0.91 - ETA: 0s - loss: 0.2986 - accuracy: 0.91 - ETA: 0s - loss: 0.2979 - accuracy: 0.91 - ETA: 0s - loss: 0.2967 - accuracy: 0.91 - ETA: 0s - loss: 0.2955 - accuracy: 0.91 - 6s 99us/sample - loss: 0.2948 - accuracy: 0.9135\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 9s - loss: 0.1886 - accuracy: 0.96 - ETA: 5s - loss: 0.1863 - accuracy: 0.94 - ETA: 5s - loss: 0.1630 - accuracy: 0.95 - ETA: 5s - loss: 0.1623 - accuracy: 0.95 - ETA: 5s - loss: 0.1623 - accuracy: 0.95 - ETA: 5s - loss: 0.1583 - accuracy: 0.95 - ETA: 5s - loss: 0.1585 - accuracy: 0.95 - ETA: 5s - loss: 0.1588 - accuracy: 0.95 - ETA: 5s - loss: 0.1561 - accuracy: 0.95 - ETA: 5s - loss: 0.1603 - accuracy: 0.95 - ETA: 4s - loss: 0.1568 - accuracy: 0.95 - ETA: 4s - loss: 0.1553 - accuracy: 0.95 - ETA: 4s - loss: 0.1544 - accuracy: 0.95 - ETA: 4s - loss: 0.1593 - accuracy: 0.95 - ETA: 4s - loss: 0.1594 - accuracy: 0.95 - ETA: 4s - loss: 0.1606 - accuracy: 0.95 - ETA: 4s - loss: 0.1604 - accuracy: 0.95 - ETA: 4s - loss: 0.1610 - accuracy: 0.95 - ETA: 4s - loss: 0.1609 - accuracy: 0.95 - ETA: 4s - loss: 0.1592 - accuracy: 0.95 - ETA: 4s - loss: 0.1589 - accuracy: 0.95 - ETA: 4s - loss: 0.1582 - accuracy: 0.95 - ETA: 4s - loss: 0.1584 - accuracy: 0.95 - ETA: 4s - loss: 0.1593 - accuracy: 0.95 - ETA: 4s - loss: 0.1601 - accuracy: 0.95 - ETA: 4s - loss: 0.1589 - accuracy: 0.95 - ETA: 4s - loss: 0.1580 - accuracy: 0.95 - ETA: 4s - loss: 0.1575 - accuracy: 0.95 - ETA: 4s - loss: 0.1572 - accuracy: 0.95 - ETA: 4s - loss: 0.1572 - accuracy: 0.95 - ETA: 4s - loss: 0.1575 - accuracy: 0.95 - ETA: 4s - loss: 0.1561 - accuracy: 0.95 - ETA: 4s - loss: 0.1559 - accuracy: 0.95 - ETA: 4s - loss: 0.1554 - accuracy: 0.95 - ETA: 4s - loss: 0.1555 - accuracy: 0.95 - ETA: 4s - loss: 0.1549 - accuracy: 0.95 - ETA: 4s - loss: 0.1544 - accuracy: 0.95 - ETA: 4s - loss: 0.1548 - accuracy: 0.95 - ETA: 4s - loss: 0.1541 - accuracy: 0.95 - ETA: 4s - loss: 0.1539 - accuracy: 0.95 - ETA: 4s - loss: 0.1538 - accuracy: 0.95 - ETA: 4s - loss: 0.1545 - accuracy: 0.95 - ETA: 4s - loss: 0.1542 - accuracy: 0.95 - ETA: 3s - loss: 0.1544 - accuracy: 0.95 - ETA: 3s - loss: 0.1542 - accuracy: 0.95 - ETA: 3s - loss: 0.1533 - accuracy: 0.95 - ETA: 3s - loss: 0.1536 - accuracy: 0.95 - ETA: 3s - loss: 0.1537 - accuracy: 0.95 - ETA: 3s - loss: 0.1533 - accuracy: 0.95 - ETA: 3s - loss: 0.1539 - accuracy: 0.95 - ETA: 3s - loss: 0.1539 - accuracy: 0.95 - ETA: 3s - loss: 0.1539 - accuracy: 0.95 - ETA: 3s - loss: 0.1533 - accuracy: 0.95 - ETA: 3s - loss: 0.1531 - accuracy: 0.95 - ETA: 3s - loss: 0.1537 - accuracy: 0.95 - ETA: 3s - loss: 0.1536 - accuracy: 0.95 - ETA: 3s - loss: 0.1540 - accuracy: 0.95 - ETA: 3s - loss: 0.1542 - accuracy: 0.95 - ETA: 3s - loss: 0.1540 - accuracy: 0.95 - ETA: 3s - loss: 0.1538 - accuracy: 0.95 - ETA: 3s - loss: 0.1541 - accuracy: 0.95 - ETA: 3s - loss: 0.1532 - accuracy: 0.95 - ETA: 2s - loss: 0.1530 - accuracy: 0.95 - ETA: 2s - loss: 0.1528 - accuracy: 0.95 - ETA: 2s - loss: 0.1525 - accuracy: 0.95 - ETA: 2s - loss: 0.1527 - accuracy: 0.95 - ETA: 2s - loss: 0.1523 - accuracy: 0.95 - ETA: 2s - loss: 0.1524 - accuracy: 0.95 - ETA: 2s - loss: 0.1524 - accuracy: 0.95 - ETA: 2s - loss: 0.1518 - accuracy: 0.95 - ETA: 2s - loss: 0.1510 - accuracy: 0.95 - ETA: 2s - loss: 0.1509 - accuracy: 0.95 - ETA: 2s - loss: 0.1504 - accuracy: 0.95 - ETA: 2s - loss: 0.1504 - accuracy: 0.95 - ETA: 2s - loss: 0.1502 - accuracy: 0.95 - ETA: 2s - loss: 0.1500 - accuracy: 0.95 - ETA: 2s - loss: 0.1492 - accuracy: 0.95 - ETA: 1s - loss: 0.1485 - accuracy: 0.95 - ETA: 1s - loss: 0.1491 - accuracy: 0.95 - ETA: 1s - loss: 0.1487 - accuracy: 0.95 - ETA: 1s - loss: 0.1481 - accuracy: 0.95 - ETA: 1s - loss: 0.1482 - accuracy: 0.95 - ETA: 1s - loss: 0.1483 - accuracy: 0.95 - ETA: 1s - loss: 0.1482 - accuracy: 0.95 - ETA: 1s - loss: 0.1475 - accuracy: 0.95 - ETA: 1s - loss: 0.1474 - accuracy: 0.95 - ETA: 1s - loss: 0.1474 - accuracy: 0.95 - ETA: 1s - loss: 0.1468 - accuracy: 0.95 - ETA: 1s - loss: 0.1463 - accuracy: 0.95 - ETA: 1s - loss: 0.1462 - accuracy: 0.95 - ETA: 1s - loss: 0.1460 - accuracy: 0.95 - ETA: 1s - loss: 0.1459 - accuracy: 0.95 - ETA: 1s - loss: 0.1457 - accuracy: 0.95 - ETA: 1s - loss: 0.1462 - accuracy: 0.95 - ETA: 0s - loss: 0.1461 - accuracy: 0.95 - ETA: 0s - loss: 0.1463 - accuracy: 0.95 - ETA: 0s - loss: 0.1458 - accuracy: 0.95 - ETA: 0s - loss: 0.1452 - accuracy: 0.95 - ETA: 0s - loss: 0.1452 - accuracy: 0.95 - ETA: 0s - loss: 0.1448 - accuracy: 0.95 - ETA: 0s - loss: 0.1448 - accuracy: 0.95 - ETA: 0s - loss: 0.1447 - accuracy: 0.95 - ETA: 0s - loss: 0.1443 - accuracy: 0.95 - ETA: 0s - loss: 0.1446 - accuracy: 0.95 - ETA: 0s - loss: 0.1444 - accuracy: 0.95 - ETA: 0s - loss: 0.1438 - accuracy: 0.95 - ETA: 0s - loss: 0.1441 - accuracy: 0.95 - ETA: 0s - loss: 0.1439 - accuracy: 0.95 - ETA: 0s - loss: 0.1437 - accuracy: 0.95 - ETA: 0s - loss: 0.1436 - accuracy: 0.95 - ETA: 0s - loss: 0.1439 - accuracy: 0.95 - 6s 95us/sample - loss: 0.1437 - accuracy: 0.9568\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 11s - loss: 0.3190 - accuracy: 0.937 - ETA: 5s - loss: 0.1060 - accuracy: 0.968 - ETA: 5s - loss: 0.1286 - accuracy: 0.96 - ETA: 5s - loss: 0.1306 - accuracy: 0.96 - ETA: 4s - loss: 0.1210 - accuracy: 0.96 - ETA: 4s - loss: 0.1159 - accuracy: 0.97 - ETA: 4s - loss: 0.1150 - accuracy: 0.96 - ETA: 4s - loss: 0.1103 - accuracy: 0.97 - ETA: 4s - loss: 0.1048 - accuracy: 0.97 - ETA: 4s - loss: 0.1068 - accuracy: 0.97 - ETA: 4s - loss: 0.1050 - accuracy: 0.97 - ETA: 4s - loss: 0.1083 - accuracy: 0.96 - ETA: 4s - loss: 0.1091 - accuracy: 0.96 - ETA: 4s - loss: 0.1122 - accuracy: 0.96 - ETA: 4s - loss: 0.1125 - accuracy: 0.96 - ETA: 4s - loss: 0.1124 - accuracy: 0.96 - ETA: 4s - loss: 0.1097 - accuracy: 0.96 - ETA: 4s - loss: 0.1096 - accuracy: 0.96 - ETA: 4s - loss: 0.1115 - accuracy: 0.96 - ETA: 4s - loss: 0.1113 - accuracy: 0.96 - ETA: 4s - loss: 0.1108 - accuracy: 0.96 - ETA: 3s - loss: 0.1103 - accuracy: 0.96 - ETA: 3s - loss: 0.1083 - accuracy: 0.96 - ETA: 3s - loss: 0.1063 - accuracy: 0.96 - ETA: 3s - loss: 0.1051 - accuracy: 0.96 - ETA: 3s - loss: 0.1063 - accuracy: 0.96 - ETA: 3s - loss: 0.1070 - accuracy: 0.96 - ETA: 3s - loss: 0.1080 - accuracy: 0.96 - ETA: 3s - loss: 0.1073 - accuracy: 0.96 - ETA: 3s - loss: 0.1072 - accuracy: 0.96 - ETA: 3s - loss: 0.1079 - accuracy: 0.96 - ETA: 3s - loss: 0.1075 - accuracy: 0.96 - ETA: 3s - loss: 0.1077 - accuracy: 0.96 - ETA: 3s - loss: 0.1088 - accuracy: 0.96 - ETA: 3s - loss: 0.1089 - accuracy: 0.96 - ETA: 3s - loss: 0.1089 - accuracy: 0.96 - ETA: 3s - loss: 0.1087 - accuracy: 0.96 - ETA: 3s - loss: 0.1093 - accuracy: 0.96 - ETA: 3s - loss: 0.1094 - accuracy: 0.96 - ETA: 2s - loss: 0.1105 - accuracy: 0.96 - ETA: 2s - loss: 0.1111 - accuracy: 0.96 - ETA: 2s - loss: 0.1112 - accuracy: 0.96 - ETA: 2s - loss: 0.1108 - accuracy: 0.96 - ETA: 2s - loss: 0.1107 - accuracy: 0.96 - ETA: 2s - loss: 0.1100 - accuracy: 0.96 - ETA: 2s - loss: 0.1094 - accuracy: 0.96 - ETA: 2s - loss: 0.1092 - accuracy: 0.96 - ETA: 2s - loss: 0.1103 - accuracy: 0.96 - ETA: 2s - loss: 0.1105 - accuracy: 0.96 - ETA: 2s - loss: 0.1102 - accuracy: 0.96 - ETA: 2s - loss: 0.1098 - accuracy: 0.96 - ETA: 2s - loss: 0.1101 - accuracy: 0.96 - ETA: 2s - loss: 0.1100 - accuracy: 0.96 - ETA: 2s - loss: 0.1101 - accuracy: 0.96 - ETA: 2s - loss: 0.1102 - accuracy: 0.96 - ETA: 2s - loss: 0.1098 - accuracy: 0.96 - ETA: 2s - loss: 0.1098 - accuracy: 0.96 - ETA: 2s - loss: 0.1095 - accuracy: 0.96 - ETA: 2s - loss: 0.1090 - accuracy: 0.96 - ETA: 1s - loss: 0.1095 - accuracy: 0.96 - ETA: 1s - loss: 0.1092 - accuracy: 0.96 - ETA: 1s - loss: 0.1089 - accuracy: 0.96 - ETA: 1s - loss: 0.1090 - accuracy: 0.96 - ETA: 1s - loss: 0.1086 - accuracy: 0.96 - ETA: 1s - loss: 0.1085 - accuracy: 0.96 - ETA: 1s - loss: 0.1086 - accuracy: 0.96 - ETA: 1s - loss: 0.1083 - accuracy: 0.96 - ETA: 1s - loss: 0.1085 - accuracy: 0.96 - ETA: 1s - loss: 0.1079 - accuracy: 0.96 - ETA: 1s - loss: 0.1080 - accuracy: 0.96 - ETA: 1s - loss: 0.1079 - accuracy: 0.96 - ETA: 1s - loss: 0.1077 - accuracy: 0.96 - ETA: 1s - loss: 0.1074 - accuracy: 0.96 - ETA: 1s - loss: 0.1073 - accuracy: 0.96 - ETA: 1s - loss: 0.1076 - accuracy: 0.96 - ETA: 1s - loss: 0.1079 - accuracy: 0.96 - ETA: 1s - loss: 0.1079 - accuracy: 0.96 - ETA: 1s - loss: 0.1074 - accuracy: 0.96 - ETA: 0s - loss: 0.1073 - accuracy: 0.96 - ETA: 0s - loss: 0.1073 - accuracy: 0.96 - ETA: 0s - loss: 0.1072 - accuracy: 0.96 - ETA: 0s - loss: 0.1073 - accuracy: 0.96 - ETA: 0s - loss: 0.1070 - accuracy: 0.96 - ETA: 0s - loss: 0.1072 - accuracy: 0.96 - ETA: 0s - loss: 0.1077 - accuracy: 0.96 - ETA: 0s - loss: 0.1076 - accuracy: 0.96 - ETA: 0s - loss: 0.1076 - accuracy: 0.96 - ETA: 0s - loss: 0.1075 - accuracy: 0.96 - ETA: 0s - loss: 0.1076 - accuracy: 0.96 - ETA: 0s - loss: 0.1077 - accuracy: 0.96 - ETA: 0s - loss: 0.1072 - accuracy: 0.96 - ETA: 0s - loss: 0.1070 - accuracy: 0.96 - ETA: 0s - loss: 0.1068 - accuracy: 0.96 - ETA: 0s - loss: 0.1066 - accuracy: 0.96 - ETA: 0s - loss: 0.1065 - accuracy: 0.96 - ETA: 0s - loss: 0.1064 - accuracy: 0.96 - 5s 82us/sample - loss: 0.1067 - accuracy: 0.9679\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 9s - loss: 0.1221 - accuracy: 0.93 - ETA: 5s - loss: 0.0946 - accuracy: 0.97 - ETA: 4s - loss: 0.0781 - accuracy: 0.97 - ETA: 4s - loss: 0.0791 - accuracy: 0.97 - ETA: 4s - loss: 0.0840 - accuracy: 0.97 - ETA: 4s - loss: 0.0857 - accuracy: 0.97 - ETA: 4s - loss: 0.0880 - accuracy: 0.97 - ETA: 4s - loss: 0.0862 - accuracy: 0.97 - ETA: 4s - loss: 0.0820 - accuracy: 0.97 - ETA: 4s - loss: 0.0820 - accuracy: 0.97 - ETA: 4s - loss: 0.0834 - accuracy: 0.97 - ETA: 4s - loss: 0.0829 - accuracy: 0.97 - ETA: 4s - loss: 0.0834 - accuracy: 0.97 - ETA: 4s - loss: 0.0831 - accuracy: 0.97 - ETA: 4s - loss: 0.0849 - accuracy: 0.97 - ETA: 4s - loss: 0.0844 - accuracy: 0.97 - ETA: 3s - loss: 0.0837 - accuracy: 0.97 - ETA: 3s - loss: 0.0846 - accuracy: 0.97 - ETA: 3s - loss: 0.0851 - accuracy: 0.97 - ETA: 3s - loss: 0.0852 - accuracy: 0.97 - ETA: 3s - loss: 0.0847 - accuracy: 0.97 - ETA: 3s - loss: 0.0859 - accuracy: 0.97 - ETA: 3s - loss: 0.0855 - accuracy: 0.97 - ETA: 3s - loss: 0.0866 - accuracy: 0.97 - ETA: 3s - loss: 0.0863 - accuracy: 0.97 - ETA: 3s - loss: 0.0863 - accuracy: 0.97 - ETA: 3s - loss: 0.0868 - accuracy: 0.97 - ETA: 3s - loss: 0.0864 - accuracy: 0.97 - ETA: 3s - loss: 0.0866 - accuracy: 0.97 - ETA: 3s - loss: 0.0856 - accuracy: 0.97 - ETA: 3s - loss: 0.0849 - accuracy: 0.97 - ETA: 3s - loss: 0.0840 - accuracy: 0.97 - ETA: 3s - loss: 0.0843 - accuracy: 0.97 - ETA: 3s - loss: 0.0840 - accuracy: 0.97 - ETA: 3s - loss: 0.0838 - accuracy: 0.97 - ETA: 3s - loss: 0.0838 - accuracy: 0.97 - ETA: 3s - loss: 0.0832 - accuracy: 0.97 - ETA: 3s - loss: 0.0848 - accuracy: 0.97 - ETA: 2s - loss: 0.0844 - accuracy: 0.97 - ETA: 2s - loss: 0.0843 - accuracy: 0.97 - ETA: 2s - loss: 0.0844 - accuracy: 0.97 - ETA: 2s - loss: 0.0839 - accuracy: 0.97 - ETA: 2s - loss: 0.0838 - accuracy: 0.97 - ETA: 2s - loss: 0.0839 - accuracy: 0.97 - ETA: 2s - loss: 0.0840 - accuracy: 0.97 - ETA: 2s - loss: 0.0843 - accuracy: 0.97 - ETA: 2s - loss: 0.0844 - accuracy: 0.97 - ETA: 2s - loss: 0.0846 - accuracy: 0.97 - ETA: 2s - loss: 0.0845 - accuracy: 0.97 - ETA: 2s - loss: 0.0844 - accuracy: 0.97 - ETA: 2s - loss: 0.0840 - accuracy: 0.97 - ETA: 2s - loss: 0.0842 - accuracy: 0.97 - ETA: 2s - loss: 0.0846 - accuracy: 0.97 - ETA: 2s - loss: 0.0852 - accuracy: 0.97 - ETA: 2s - loss: 0.0854 - accuracy: 0.97 - ETA: 2s - loss: 0.0852 - accuracy: 0.97 - ETA: 2s - loss: 0.0852 - accuracy: 0.97 - ETA: 2s - loss: 0.0850 - accuracy: 0.97 - ETA: 2s - loss: 0.0851 - accuracy: 0.97 - ETA: 1s - loss: 0.0851 - accuracy: 0.97 - ETA: 1s - loss: 0.0847 - accuracy: 0.97 - ETA: 1s - loss: 0.0845 - accuracy: 0.97 - ETA: 1s - loss: 0.0844 - accuracy: 0.97 - ETA: 1s - loss: 0.0846 - accuracy: 0.97 - ETA: 1s - loss: 0.0845 - accuracy: 0.97 - ETA: 1s - loss: 0.0847 - accuracy: 0.97 - ETA: 1s - loss: 0.0852 - accuracy: 0.97 - ETA: 1s - loss: 0.0852 - accuracy: 0.97 - ETA: 1s - loss: 0.0856 - accuracy: 0.97 - ETA: 1s - loss: 0.0857 - accuracy: 0.97 - ETA: 1s - loss: 0.0858 - accuracy: 0.97 - ETA: 1s - loss: 0.0860 - accuracy: 0.97 - ETA: 1s - loss: 0.0860 - accuracy: 0.97 - ETA: 1s - loss: 0.0861 - accuracy: 0.97 - ETA: 1s - loss: 0.0861 - accuracy: 0.97 - ETA: 1s - loss: 0.0865 - accuracy: 0.97 - ETA: 1s - loss: 0.0865 - accuracy: 0.97 - ETA: 1s - loss: 0.0866 - accuracy: 0.97 - ETA: 1s - loss: 0.0867 - accuracy: 0.97 - ETA: 1s - loss: 0.0867 - accuracy: 0.97 - ETA: 0s - loss: 0.0869 - accuracy: 0.97 - ETA: 0s - loss: 0.0869 - accuracy: 0.97 - ETA: 0s - loss: 0.0868 - accuracy: 0.97 - ETA: 0s - loss: 0.0869 - accuracy: 0.97 - ETA: 0s - loss: 0.0867 - accuracy: 0.97 - ETA: 0s - loss: 0.0867 - accuracy: 0.97 - ETA: 0s - loss: 0.0872 - accuracy: 0.97 - ETA: 0s - loss: 0.0876 - accuracy: 0.97 - ETA: 0s - loss: 0.0882 - accuracy: 0.97 - ETA: 0s - loss: 0.0879 - accuracy: 0.97 - ETA: 0s - loss: 0.0881 - accuracy: 0.97 - ETA: 0s - loss: 0.0880 - accuracy: 0.97 - ETA: 0s - loss: 0.0882 - accuracy: 0.97 - ETA: 0s - loss: 0.0884 - accuracy: 0.97 - ETA: 0s - loss: 0.0883 - accuracy: 0.97 - ETA: 0s - loss: 0.0883 - accuracy: 0.97 - ETA: 0s - loss: 0.0881 - accuracy: 0.97 - ETA: 0s - loss: 0.0881 - accuracy: 0.97 - 5s 84us/sample - loss: 0.0882 - accuracy: 0.9730\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - ETA: 7s - loss: 0.0087 - accuracy: 1.00 - ETA: 4s - loss: 0.0572 - accuracy: 0.98 - ETA: 4s - loss: 0.0609 - accuracy: 0.97 - ETA: 4s - loss: 0.0617 - accuracy: 0.97 - ETA: 4s - loss: 0.0702 - accuracy: 0.97 - ETA: 4s - loss: 0.0708 - accuracy: 0.97 - ETA: 4s - loss: 0.0690 - accuracy: 0.97 - ETA: 4s - loss: 0.0676 - accuracy: 0.97 - ETA: 4s - loss: 0.0697 - accuracy: 0.97 - ETA: 4s - loss: 0.0693 - accuracy: 0.97 - ETA: 4s - loss: 0.0694 - accuracy: 0.97 - ETA: 4s - loss: 0.0695 - accuracy: 0.97 - ETA: 4s - loss: 0.0685 - accuracy: 0.97 - ETA: 4s - loss: 0.0693 - accuracy: 0.97 - ETA: 4s - loss: 0.0691 - accuracy: 0.97 - ETA: 4s - loss: 0.0691 - accuracy: 0.97 - ETA: 4s - loss: 0.0685 - accuracy: 0.97 - ETA: 3s - loss: 0.0682 - accuracy: 0.97 - ETA: 3s - loss: 0.0674 - accuracy: 0.97 - ETA: 3s - loss: 0.0677 - accuracy: 0.97 - ETA: 3s - loss: 0.0686 - accuracy: 0.97 - ETA: 3s - loss: 0.0686 - accuracy: 0.97 - ETA: 3s - loss: 0.0673 - accuracy: 0.97 - ETA: 3s - loss: 0.0673 - accuracy: 0.97 - ETA: 3s - loss: 0.0671 - accuracy: 0.97 - ETA: 3s - loss: 0.0666 - accuracy: 0.97 - ETA: 3s - loss: 0.0663 - accuracy: 0.97 - ETA: 3s - loss: 0.0667 - accuracy: 0.97 - ETA: 3s - loss: 0.0667 - accuracy: 0.97 - ETA: 3s - loss: 0.0662 - accuracy: 0.97 - ETA: 3s - loss: 0.0663 - accuracy: 0.97 - ETA: 3s - loss: 0.0665 - accuracy: 0.97 - ETA: 3s - loss: 0.0671 - accuracy: 0.97 - ETA: 3s - loss: 0.0678 - accuracy: 0.97 - ETA: 3s - loss: 0.0679 - accuracy: 0.97 - ETA: 3s - loss: 0.0677 - accuracy: 0.97 - ETA: 3s - loss: 0.0677 - accuracy: 0.97 - ETA: 3s - loss: 0.0676 - accuracy: 0.97 - ETA: 3s - loss: 0.0684 - accuracy: 0.97 - ETA: 3s - loss: 0.0686 - accuracy: 0.97 - ETA: 3s - loss: 0.0694 - accuracy: 0.97 - ETA: 3s - loss: 0.0699 - accuracy: 0.97 - ETA: 3s - loss: 0.0692 - accuracy: 0.97 - ETA: 3s - loss: 0.0698 - accuracy: 0.97 - ETA: 2s - loss: 0.0698 - accuracy: 0.97 - ETA: 2s - loss: 0.0705 - accuracy: 0.97 - ETA: 2s - loss: 0.0709 - accuracy: 0.97 - ETA: 2s - loss: 0.0719 - accuracy: 0.97 - ETA: 2s - loss: 0.0720 - accuracy: 0.97 - ETA: 2s - loss: 0.0723 - accuracy: 0.97 - ETA: 2s - loss: 0.0725 - accuracy: 0.97 - ETA: 2s - loss: 0.0725 - accuracy: 0.97 - ETA: 2s - loss: 0.0724 - accuracy: 0.97 - ETA: 2s - loss: 0.0730 - accuracy: 0.97 - ETA: 2s - loss: 0.0733 - accuracy: 0.97 - ETA: 2s - loss: 0.0731 - accuracy: 0.97 - ETA: 2s - loss: 0.0731 - accuracy: 0.97 - ETA: 2s - loss: 0.0729 - accuracy: 0.97 - ETA: 2s - loss: 0.0725 - accuracy: 0.97 - ETA: 2s - loss: 0.0723 - accuracy: 0.97 - ETA: 2s - loss: 0.0728 - accuracy: 0.97 - ETA: 2s - loss: 0.0725 - accuracy: 0.97 - ETA: 2s - loss: 0.0731 - accuracy: 0.97 - ETA: 2s - loss: 0.0739 - accuracy: 0.97 - ETA: 1s - loss: 0.0737 - accuracy: 0.97 - ETA: 1s - loss: 0.0736 - accuracy: 0.97 - ETA: 1s - loss: 0.0736 - accuracy: 0.97 - ETA: 1s - loss: 0.0738 - accuracy: 0.97 - ETA: 1s - loss: 0.0739 - accuracy: 0.97 - ETA: 1s - loss: 0.0740 - accuracy: 0.97 - ETA: 1s - loss: 0.0738 - accuracy: 0.97 - ETA: 1s - loss: 0.0744 - accuracy: 0.97 - ETA: 1s - loss: 0.0746 - accuracy: 0.97 - ETA: 1s - loss: 0.0747 - accuracy: 0.97 - ETA: 1s - loss: 0.0748 - accuracy: 0.97 - ETA: 1s - loss: 0.0748 - accuracy: 0.97 - ETA: 1s - loss: 0.0749 - accuracy: 0.97 - ETA: 1s - loss: 0.0750 - accuracy: 0.97 - ETA: 1s - loss: 0.0755 - accuracy: 0.97 - ETA: 1s - loss: 0.0754 - accuracy: 0.97 - ETA: 1s - loss: 0.0752 - accuracy: 0.97 - ETA: 1s - loss: 0.0753 - accuracy: 0.97 - ETA: 0s - loss: 0.0754 - accuracy: 0.97 - ETA: 0s - loss: 0.0753 - accuracy: 0.97 - ETA: 0s - loss: 0.0751 - accuracy: 0.97 - ETA: 0s - loss: 0.0750 - accuracy: 0.97 - ETA: 0s - loss: 0.0748 - accuracy: 0.97 - ETA: 0s - loss: 0.0748 - accuracy: 0.97 - ETA: 0s - loss: 0.0750 - accuracy: 0.97 - ETA: 0s - loss: 0.0752 - accuracy: 0.97 - ETA: 0s - loss: 0.0751 - accuracy: 0.97 - ETA: 0s - loss: 0.0749 - accuracy: 0.97 - ETA: 0s - loss: 0.0748 - accuracy: 0.97 - ETA: 0s - loss: 0.0747 - accuracy: 0.97 - ETA: 0s - loss: 0.0755 - accuracy: 0.97 - ETA: 0s - loss: 0.0752 - accuracy: 0.97 - ETA: 0s - loss: 0.0751 - accuracy: 0.97 - ETA: 0s - loss: 0.0752 - accuracy: 0.97 - ETA: 0s - loss: 0.0754 - accuracy: 0.97 - ETA: 0s - loss: 0.0752 - accuracy: 0.97 - 5s 86us/sample - loss: 0.0754 - accuracy: 0.9765\n",
      "10000/1 - 1s - loss: 0.0376 - accuracy: 0.9773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07306932950122282, 0.9773]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One epoch is the time step that is incremented every time it has went through all the samples in the training set.\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history=model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "#Computer's prediction\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is me trying to figure out how predictive models work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lengthinfeet</th>\n",
       "      <th>weightintons</th>\n",
       "      <th>speedmph</th>\n",
       "      <th>biteforcenewtons</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>57000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>60.00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>182000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>43.0</td>\n",
       "      <td>8.60</td>\n",
       "      <td>31.0</td>\n",
       "      <td>3340</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>42.6</td>\n",
       "      <td>9.75</td>\n",
       "      <td>17.0</td>\n",
       "      <td>56900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>44.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>156001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lengthinfeet  weightintons  speedmph  biteforcenewtons  Species\n",
       "0          40.0          7.00      18.0             57000        0\n",
       "1          21.0         60.00      20.0            182000        1\n",
       "2          43.0          8.60      31.0              3340        2\n",
       "3          42.6          9.75      17.0             56900        0\n",
       "4          44.0         50.00      19.0            156001        1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#use one-word names for columns or it will give you an error\n",
    "CSV_COLUMN_NAMES = ['lengthinfeet', 'weightintons', 'speedmph', 'biteforcenewtons', 'Species']\n",
    "SPECIES = ['T-rex', 'Megalodon', 'Giganotosaurus']\n",
    "#Make sure the \n",
    "train = pd.read_csv('Dino_train.csv', names=CSV_COLUMN_NAMES, skipinitialspace=True, skiprows=1, engine=\"python\")\n",
    "test = pd.read_csv('Dino_test.csv', names=CSV_COLUMN_NAMES, skipinitialspace=True, skiprows=1, engine=\"python\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\WALLAC~1\\AppData\\Local\\Temp\\tmpwebh17ut\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\WALLAC~1\\\\AppData\\\\Local\\\\Temp\\\\tmpwebh17ut', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001B7FF370F48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "train_y = train.pop('Species')\n",
    "test_y = test.pop('Species')\n",
    "\n",
    "def input_evaluation_set():\n",
    "    features = {'lengthinfeet': np.array([40.0, 21.0]),\n",
    "                'weightintons':  np.array([7.00, 60.00]),\n",
    "                'speedmph': np.array([18.0, 20.0]),\n",
    "                'biteforcenewtons':  np.array([57000, 182000])}\n",
    "    labels = np.array([2, 1])\n",
    "    return features, labels\n",
    "def input_fn(features, labels, training=True, batch_size=256):\n",
    "    \"\"\"An input function for training or evaluating\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle and repeat if you are in training mode.\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "    \n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "# Feature columns describe how to use the input.\n",
    "my_feature_columns = []\n",
    "for key in train.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 30 and 10 nodes respectively.\n",
    "    hidden_units=[30, 10],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\WALLAC~1\\AppData\\Local\\Temp\\tmpwebh17ut\\model.ckpt.\n",
      "INFO:tensorflow:loss = 15699.132, step = 0\n",
      "INFO:tensorflow:global_step/sec: 273.999\n",
      "INFO:tensorflow:loss = 15.522938, step = 100 (0.366 sec)\n",
      "INFO:tensorflow:global_step/sec: 484.66\n",
      "INFO:tensorflow:loss = 16.750416, step = 200 (0.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 489.109\n",
      "INFO:tensorflow:loss = 18.80822, step = 300 (0.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 501.339\n",
      "INFO:tensorflow:loss = 10.030792, step = 400 (0.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 498.838\n",
      "INFO:tensorflow:loss = 6.8990088, step = 500 (0.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 501.342\n",
      "INFO:tensorflow:loss = 14.522602, step = 600 (0.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 491.508\n",
      "INFO:tensorflow:loss = 5.9752846, step = 700 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 493.928\n",
      "INFO:tensorflow:loss = 5.338814, step = 800 (0.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 503.732\n",
      "INFO:tensorflow:loss = 9.79402, step = 900 (0.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 501.462\n",
      "INFO:tensorflow:loss = 5.0665016, step = 1000 (0.199 sec)\n",
      "INFO:tensorflow:global_step/sec: 501.337\n",
      "INFO:tensorflow:loss = 4.662503, step = 1100 (0.199 sec)\n",
      "INFO:tensorflow:global_step/sec: 506.402\n",
      "INFO:tensorflow:loss = 3.796965, step = 1200 (0.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 493.925\n",
      "INFO:tensorflow:loss = 3.6830566, step = 1300 (0.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 489.108\n",
      "INFO:tensorflow:loss = 9.20999, step = 1400 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 489.113\n",
      "INFO:tensorflow:loss = 3.7848644, step = 1500 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 397.888\n",
      "INFO:tensorflow:loss = 2.4678545, step = 1600 (0.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 393.204\n",
      "INFO:tensorflow:loss = 2.1902509, step = 1700 (0.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 345.128\n",
      "INFO:tensorflow:loss = 1.998354, step = 1800 (0.291 sec)\n",
      "INFO:tensorflow:global_step/sec: 412.623\n",
      "INFO:tensorflow:loss = 4.454052, step = 1900 (0.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 527.583\n",
      "INFO:tensorflow:loss = 1.304955, step = 2000 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 446.558\n",
      "INFO:tensorflow:loss = 1.3400729, step = 2100 (0.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 486.104\n",
      "INFO:tensorflow:loss = 1.2738206, step = 2200 (0.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 493.934\n",
      "INFO:tensorflow:loss = 0.69274807, step = 2300 (0.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 493.929\n",
      "INFO:tensorflow:loss = 0.7554461, step = 2400 (0.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 493.928\n",
      "INFO:tensorflow:loss = 0.66821986, step = 2500 (0.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 489.107\n",
      "INFO:tensorflow:loss = 0.36972407, step = 2600 (0.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 486.738\n",
      "INFO:tensorflow:loss = 0.31927714, step = 2700 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 491.497\n",
      "INFO:tensorflow:loss = 0.2713769, step = 2800 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 501.345\n",
      "INFO:tensorflow:loss = 0.23028946, step = 2900 (0.199 sec)\n",
      "INFO:tensorflow:global_step/sec: 493.932\n",
      "INFO:tensorflow:loss = 1.039796, step = 3000 (0.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 501.339\n",
      "INFO:tensorflow:loss = 0.17720377, step = 3100 (0.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 489.109\n",
      "INFO:tensorflow:loss = 0.16494, step = 3200 (0.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 496.371\n",
      "INFO:tensorflow:loss = 0.14788234, step = 3300 (0.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 501.341\n",
      "INFO:tensorflow:loss = 0.14482364, step = 3400 (0.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 493.929\n",
      "INFO:tensorflow:loss = 0.13300592, step = 3500 (0.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 489.108\n",
      "INFO:tensorflow:loss = 0.12348532, step = 3600 (0.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 496.372\n",
      "INFO:tensorflow:loss = 0.11446107, step = 3700 (0.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 526.555\n",
      "INFO:tensorflow:loss = 0.11229396, step = 3800 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 500.037\n",
      "INFO:tensorflow:loss = 0.10629128, step = 3900 (0.200 sec)\n",
      "INFO:tensorflow:global_step/sec: 453.701\n",
      "INFO:tensorflow:loss = 0.100282006, step = 4000 (0.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 549.809\n",
      "INFO:tensorflow:loss = 0.091291636, step = 4100 (0.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 438.364\n",
      "INFO:tensorflow:loss = 0.087677546, step = 4200 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 486.733\n",
      "INFO:tensorflow:loss = 0.08420022, step = 4300 (0.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 496.376\n",
      "INFO:tensorflow:loss = 0.08442772, step = 4400 (0.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 475.122\n",
      "INFO:tensorflow:loss = 0.07890192, step = 4500 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 409.31\n",
      "INFO:tensorflow:loss = 0.0735071, step = 4600 (0.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 514.198\n",
      "INFO:tensorflow:loss = 0.07132228, step = 4700 (0.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 459.944\n",
      "INFO:tensorflow:loss = 0.069217294, step = 4800 (0.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 491.503\n",
      "INFO:tensorflow:loss = 0.06912634, step = 4900 (0.202 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5000 into C:\\Users\\WALLAC~1\\AppData\\Local\\Temp\\tmpwebh17ut\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.064096905.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-01-09T21:03:59Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\WALLAC~1\\AppData\\Local\\Temp\\tmpwebh17ut\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2020-01-09-21:03:59\n",
      "INFO:tensorflow:Saving dict for global step 5000: accuracy = 1.0, average_loss = 0.06271482, global_step = 5000, loss = 0.06271482\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: C:\\Users\\WALLAC~1\\AppData\\Local\\Temp\\tmpwebh17ut\\model.ckpt-5000\n",
      "\n",
      "Test set accuracy: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the Model.\n",
    "classifier.train(\n",
    "    input_fn=lambda: input_fn(train, train_y, training=True),\n",
    "    steps=5000)\n",
    "\n",
    "eval_result = classifier.evaluate(\n",
    "input_fn=lambda: input_fn(test, test_y, training=False))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\WALLAC~1\\AppData\\Local\\Temp\\tmpwebh17ut\\model.ckpt-5000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Prediction is \"T-rex\" (92.3%), expected \"T-rex\"\n",
      "Prediction is \"Megalodon\" (99.6%), expected \"Megalodon\"\n",
      "Prediction is \"Giganotosaurus\" (99.5%), expected \"Giganotosaurus\"\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions from the model\n",
    "expected = ['T-rex', 'Megalodon', 'Giganotosaurus']\n",
    "predict_x = {\n",
    "    'lengthinfeet': [41.5, 23, 44],\n",
    "    'weightintons': [7.1, 59.3, 8.4],\n",
    "    'speedmph': [18.6, 20.6, 30],\n",
    "    'biteforcenewtons': [57023, 182023, 3342],\n",
    "}\n",
    "\n",
    "def input_fn(features, batch_size=256):\n",
    "    \"\"\"An input function for prediction.\"\"\"\n",
    "    # Convert the inputs to a Dataset without labels.\n",
    "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
    "ar=[]\n",
    "xpec=[]\n",
    "predictions = classifier.predict(\n",
    "    input_fn=lambda: input_fn(predict_x))\n",
    "for pred_dict, expec in zip(predictions, expected):\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print('Prediction is \"{}\" ({:.1f}%), expected \"{}\"'.format(\n",
    "        SPECIES[class_id], 100 * probability, expec))\n",
    "    ar.append(probability*100)\n",
    "    xpec.append(expec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[92.28587746620178, 99.63579773902893, 99.47457313537598]\n"
     ]
    }
   ],
   "source": [
    "print(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "objects = ('T-rex, expected:\"{}\"'.format(xpec[0]), 'Megalodon, expected:\"{}\"'.format(xpec[1]), 'Giganotosaurus, expected:\"{}\"'.format(xpec[2]))\n",
    "y_pos = np.arange(len(objects))\n",
    "plt.rcParams[\"figure.figsize\"] = (18,5)\n",
    "plt.bar(y_pos, ar, align='center', alpha=1)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.xlabel('Speices')\n",
    "plt.ylabel('Confidence precentage')\n",
    "plt.title('Predictions of the Classification model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
